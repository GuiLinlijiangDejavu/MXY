# -*- coding: utf-8 -*-
"""
"""
import pandas as pd
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import re
import jieba
import streamlit as st
import os

class ContentBasedNewsRecommender:
    def __init__(self, data_paths=None, stopwords_path=None):
        self.data = None
        self.tfidf_vectorizer = None
        self.news_vectors = None
        self.data_paths = data_paths
        self.stopwords = self._load_stopwords(stopwords_path)

    def _load_stopwords(self, stopwords_path=None):
        """åŠ è½½åœç”¨è¯è¡¨"""
        default_stopwords = {'çš„', 'äº†', 'åœ¨', 'æ˜¯', 'æˆ‘', 'æœ‰', 'å’Œ', 'å°±', 'ä¸', 'äºº', 'éƒ½', 'ä¸€', 'ä¸€ä¸ª', 'ä¸Š', 'ä¹Ÿ', 'å¾ˆ', 'åˆ°', 'è¯´', 'è¦', 'å»', 'ä½ ', 'ä¼š', 'ç€', 'æ²¡æœ‰', 'çœ‹', 'å¥½', 'è‡ªå·±', 'è¿™'}
        return default_stopwords

    def load_data(self, data_paths=None):
        data_paths = data_paths or self.data_paths
        if not data_paths:
            raise ValueError("è¯·æä¾›æ•°æ®è·¯å¾„")

        all_data = []
        for data_path in data_paths:
            print(f"åŠ è½½æ•°æ®: {data_path}")
            data = pd.read_csv(data_path, encoding='utf-8')
            all_data.append(data)

        # åˆå¹¶æ‰€æœ‰æ•°æ®
        self.data = pd.concat(all_data, ignore_index=True)

        # æ•°æ®æ¸…æ´—ï¼šç¡®ä¿æ ‡é¢˜ã€æ‘˜è¦ã€å…³é”®è¯å’Œé“¾æ¥ä¸ä¸ºç©º
        print("æ•°æ®æ¸…æ´—ä¸­...")
        required_columns = ['æ ‡é¢˜', 'æ‘˜è¦', 'å…³é”®è¯', 'é“¾æ¥']
        self.data = self.data.dropna(subset=required_columns)

        # æ–‡æœ¬é¢„å¤„ç†ï¼šåˆå¹¶æ ‡é¢˜ã€æ‘˜è¦ã€å…³é”®è¯å’Œæ­£æ–‡
        print("æ–‡æœ¬é¢„å¤„ç†ä¸­...")
        self.data['combined_features'] = self.data['æ ‡é¢˜'] + " " + self.data['æ‘˜è¦'] + " " + self.data['å…³é”®è¯'] + " " + self.data['æ­£æ–‡å†…å®¹']
        self.data['clean_features'] = self.data['combined_features'].apply(self._clean_text).apply(self._remove_stopwords).apply(self._segment_text)

        print(f"æ•°æ®åŠ è½½å®Œæˆï¼Œå…± {len(self.data)} æ¡æ–°é—»")
        return self

    def train(self):
        if self.data is None:
            raise ValueError("è¯·å…ˆåŠ è½½æ•°æ®")

        print("è®­ç»ƒTF-IDFæ¨¡å‹...")
        self.tfidf_vectorizer = TfidfVectorizer(max_features=20000, ngram_range=(1, 2))  # å¢åŠ ç‰¹å¾ç»´åº¦å’Œn-gram
        self.news_vectors = self.tfidf_vectorizer.fit_transform(self.data['clean_features'])

        print("æ¨¡å‹è®­ç»ƒå®Œæˆ")
        return self

    def recommend(self, user_input, top_n=5):
        if self.data is None or self.tfidf_vectorizer is None:
            raise ValueError("æ¨¡å‹æœªåˆå§‹åŒ–ï¼Œè¯·å…ˆåŠ è½½æ•°æ®å¹¶è®­ç»ƒ")

        # å¤„ç†ç”¨æˆ·è¾“å…¥
        clean_input = self._segment_text(self._remove_stopwords(self._clean_text(user_input)))
        user_vector = self.tfidf_vectorizer.transform([clean_input])

        # è®¡ç®—ç›¸ä¼¼åº¦
        similarities = cosine_similarity(user_vector, self.news_vectors).flatten()
        top_indices = similarities.argsort()[::-1][:top_n]

        # æ„å»ºæ¨èç»“æœ
        recommendations = []
        for idx in top_indices:
            recommendations.append({
                'news_id': idx,
                'title': self.data.iloc[idx]['æ ‡é¢˜'],
                'abstract': self.data.iloc[idx]['æ‘˜è¦'],
                'keywords': self.data.iloc[idx]['å…³é”®è¯'],
                'url': self.data.iloc[idx]['é“¾æ¥'],
                'similarity': float(similarities[idx]),
                'publish_time': str(self.data.iloc[idx]['å‘å¸ƒæ—¶é—´']) if 'å‘å¸ƒæ—¶é—´' in self.data.columns else 'æœªçŸ¥'
            })

        return recommendations

    def _clean_text(self, text):
        """æ¸…æ´—æ–‡æœ¬ï¼šå»é™¤ç‰¹æ®Šå­—ç¬¦ã€æ•°å­—å’Œè‹±æ–‡"""
        # ä¿ç•™ä¸­æ–‡ã€åŸºæœ¬æ ‡ç‚¹ç¬¦å·å’Œç©ºæ ¼
        pattern = re.compile(r'[^ \u4e00-\u9fa5ï¼Œã€‚ã€ï¼›ï¼šï¼ï¼Ÿï¼ˆï¼‰ã€ã€‘ã€Šã€‹ã€Œã€]')
        return re.sub(pattern, '', text).strip()

    def _remove_stopwords(self, text):
        """å»é™¤åœç”¨è¯"""
        if not text:
            return ''

        # åˆ†è¯åè¿‡æ»¤åœç”¨è¯
        words = jieba.lcut(text)
        filtered_words = [word for word in words if word not in self.stopwords]
        return ''.join(filtered_words)

    def _segment_text(self, text):
        """ä¸­æ–‡åˆ†è¯"""
        return ' '.join(jieba.cut(text, cut_all=False))

def main():
    st.set_page_config(page_title="å¤šç‰¹å¾æ–°é—»æ¨èç³»ç»Ÿ", page_icon="ğŸ“°", layout="wide")
    st.title("ğŸ“° æ–°æµªæ–°é—»å¤šç‰¹å¾æ–°é—»æ¨èç³»ç»Ÿ")
    st.image("æ–°æµªæ–°é—».jpg", use_container_width=True, caption="æ–°é—»æ¨èç³»ç»Ÿå°é¢")
    st.markdown("åŸºäºæ ‡é¢˜ã€æ‘˜è¦ã€å…³é”®è¯å’Œæ­£æ–‡çš„æ™ºèƒ½æ¨èå¼•æ“")

    with st.sidebar:
        top_n = st.slider("æ¨èæ•°é‡", 1, 30, 10)
        st.info("""
        æœ¬ç³»ç»Ÿç»“åˆæ–°é—»æ ‡é¢˜ã€æ‘˜è¦ã€å…³é”®è¯å’Œæ­£æ–‡è¿›è¡Œæ¨èï¼š
        - æ ‡é¢˜ï¼šç²¾å‡†å®šä½ä¸»é¢˜
        - æ‘˜è¦ï¼šæç‚¼æ ¸å¿ƒå†…å®¹
        - å…³é”®è¯ï¼šèšç„¦å…³é”®ä¿¡æ¯
        - æ­£æ–‡ï¼š å®¹çº³æ‰€æœ‰è¦ç´ 
        """)

    col1, col2 = st.columns([3, 1])

    with col1:
        user_input = st.text_area("è¯·è¾“å…¥æ„Ÿå…´è¶£çš„å†…å®¹",
                                  placeholder="ä¾‹å¦‚ï¼šäººå·¥æ™ºèƒ½ï¼Œå·´ä»¥å†²çª",
                                  height=100)

        if st.button("è·å–æ¨è"):
            if not user_input.strip():
                st.warning("è¯·è¾“å…¥å†…å®¹")
                return

            try:
                # åˆå§‹åŒ–å¹¶ç¼“å­˜æ¨¡å‹
                if 'recommender' not in st.session_state:
                    st.session_state.recommender = ContentBasedNewsRecommender(["T1.csv", "T2.csv"], "stopwords.txt")
                    st.session_state.recommender.load_data().train()

                # è·å–æ¨èç»“æœ
                recommendations = st.session_state.recommender.recommend(user_input, top_n=top_n)

                if recommendations:
                    st.subheader(f"ä¸ºä½ æ¨è {len(recommendations)} æ¡ç›¸å…³æ–°é—»ï¼š")

                    for i, news in enumerate(recommendations, 1):
                        with st.container():
                            st.markdown(f"### {i}. {news['title']}")

                            # å±•ç¤ºå¤šç‰¹å¾ä¿¡æ¯
                            st.markdown(f"**å…³é”®è¯**ï¼š{news['keywords']}")
                            st.markdown(f"**æ‘˜è¦**ï¼š{news['abstract']}")
                            st.markdown(f"**ç›¸ä¼¼åº¦**ï¼š{news['similarity']:.2%}")
                            st.markdown(f"**å‘å¸ƒæ—¶é—´**ï¼š{news['publish_time']}")
                            st.markdown(f"[æŸ¥çœ‹åŸæ–‡]({news['url']})", unsafe_allow_html=True)
                            st.markdown("---")
                else:
                    st.info("æœªæ‰¾åˆ°ç›¸å…³æ–°é—»ï¼Œè¯·è°ƒæ•´è¾“å…¥")
            except Exception as e:
                st.error(f"é”™è¯¯ï¼š{str(e)}")

    with col2:
        st.header("ä½¿ç”¨æç¤º")
        st.markdown("- ç›®å‰å…±æ‘˜å½•æ–°æµªæ–°é—»è´¢ç»ã€å›½å†…ã€å›½é™…ã€ä½“å½©ã€å¨±ä¹äº”ä¸ªé¢‘é“çº¦ä¸€ä¸‡ä¸¤åƒæ¡æ–°é—»")
        st.markdown("- è¾“å…¥è¶Šè¯¦ç»†ï¼Œæ¨èè¶Šç²¾å‡†")
        st.markdown("- æ”¯æŒä¸­æ–‡å…³é”®è¯ã€å¥å­æˆ–æ®µè½")
        st.markdown("- ç³»ç»Ÿä¼šåŒæ—¶åŒ¹é…æ ‡é¢˜ã€æ‘˜è¦ã€å…³é”®è¯å’Œæ­£æ–‡")

if __name__ == "__main__":
    main()